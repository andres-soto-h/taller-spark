{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este repo es una copia, el original lo puedes encontrar en:\n",
    "\n",
    "https://github.com/ayushdixit487/Uber-Data-Analysis-Project-in-Pyspark\n",
    "\n",
    "https://medium.com/towards-data-engineering/uber-data-analysis-project-in-pyspark-e105a445fc3e\n",
    "\n",
    "# Uber-Data-Analysis-Project-in-Pyspark\n",
    "<img width=\"374\" alt=\"image\" src=\"https://user-images.githubusercontent.com/25612446/219968335-5e40c842-56cd-4f87-97b8-bb304a8a4c69.png\">\n",
    "\n",
    "This data project can be used as a take-home assignment to learn Pyspark and Data Engineering.\n",
    "\n",
    "## Insights from City Supply and Demand Data\n",
    "\n",
    "## Data Description\n",
    "\n",
    "To answer the question, use the dataset from the file dataset.csv. For example, consider a row from this dataset:\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/25612446/219965433-956a1dc0-2acf-4d0d-b1cb-40723249d349.png)\n",
    "\n",
    "\n",
    "This means that during the hour beginning at 4pm (hour 16), on September 10th, 2012, 11 people opened the Uber app (Eyeballs). 2 of them did not see any car (Zeroes) and 4 of them requested a car (Requests). Of the 4 requests, only 3 complete trips actually resulted (Completed Trips). During this time, there were a total of 6 drivers who logged in (Unique Drivers).\n",
    "\n",
    "## Actividad\n",
    "\n",
    "Usando el dataset, responder a las siguientes preguntas\n",
    "\n",
    "\n",
    "⚡ ¿Qué fecha tuvo la mayoría de los viajes completados durante el período de dos semanas?\n",
    "\n",
    "⚡ ¿Cuál fue el número más alto de viajes completados dentro de un período de 24 horas?\n",
    "\n",
    "⚡ ¿Qué hora del día tuvo más solicitudes durante el período de dos semanas?\n",
    "\n",
    "⚡ ¿Qué porcentaje de todos los ceros durante el período de dos semanas ocurrieron durante el fin de semana (viernes a las 5 pm hasta el domingo a las 3 am)? Consejo: El valor de la hora local es el inicio de la hora (por ejemplo, 15 es la hora de 3:00 pm a 4:00 pm).\n",
    "\n",
    "⚡ ¿Cuál es la proporción promedio ponderada de viajes completados por conductor durante el período de dos semanas? Consejo: \"Promedio ponderado\" significa que tu respuesta debe tener en cuenta el volumen total de viajes en cada hora para determinar el número más preciso en todo el período.\n",
    "\n",
    "⚡ Al redactar un horario de conductor en términos de turnos de 8 horas, ¿cuándo son las 8 horas consecutivas más ocupadas durante el período de dos semanas en términos de solicitudes únicas? Un nuevo turno comienza cada 8 horas. Supón que un conductor trabajará el mismo turno cada día.\n",
    "\n",
    "⚡ Verdadero o Falso: ¿La oferta de conductores siempre aumenta cuando aumenta la demanda durante el período de dos semanas? Consejo: Visualiza los datos para confirmar tu respuesta si es necesario.\n",
    "\n",
    "⚡ ¿En qué período de 72 horas es la proporción de Ceros a Miradas más alta?\n",
    "\n",
    "⚡ Si pudieras agregar 5 conductores a cualquier hora única de cada día durante el período de dos semanas, ¿a qué hora deberías agregarlos? Pista: Considera tanto las miradas de los pasajeros como la oferta de conductores al elegir.\n",
    "\n",
    "⚡ Al observar los datos de todas las dos semanas, ¿qué hora podría tener más sentido considerar como un verdadero \"fin de día\" en lugar de la medianoche? (es decir, ¿cuándo la oferta y la demanda están en sus mínimos naturales?) Consejo: Visualiza los datos para confirmar tu respuesta si es necesario.\n",
    "\n",
    "\n",
    "Fuente: https://medium.com/towards-data-engineering/uber-data-analysis-project-in-pyspark-e105a445fc3e\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/03/01 14:05:39 WARN Utils: Your hostname, ANDRES-SOTOH resolves to a loopback address: 127.0.1.1; using 172.20.226.32 instead (on interface eth0)\n",
      "24/03/01 14:05:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/01 14:05:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/01 14:05:43 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.\n",
      "24/03/01 14:05:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"My App\") \\\n",
    "   .config(\"spark.driver.bindAddress\", \"172.17.0.1\",)\\\n",
    "   .config(\"spark.driver.port\", \"7077\")\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Read the data from CSV file\n",
    "uber = spark.read.csv(\"dataset.csv\", header=True, inferSchema=True, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber = uber.withColumn(\"datetime\", to_timestamp(\"Time (Local)\", \"dd-MMM-yyyy HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time (Local): string (nullable = true)\n",
      " |-- Eyeballs: integer (nullable = true)\n",
      " |-- Zeroes: integer (nullable = true)\n",
      " |-- Completed Trips: integer (nullable = true)\n",
      " |-- Requests: integer (nullable = true)\n",
      " |-- Unique Drivers: integer (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uber.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-Sep-12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group the data by date and sum the completed trips\n",
    "completed_trips_by_date = uber.groupBy(\"Date\").sum(\"Completed Trips\")\n",
    "\n",
    "# Find the date with the most completed trips\n",
    "date_with_most_completed_trips = completed_trips_by_date \\\n",
    "    .orderBy(\"sum(Completed Trips)\", ascending=False) \\\n",
    "    .select(\"Date\") \\\n",
    "    .first()[\"Date\"]\n",
    "\n",
    "print(date_with_most_completed_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------+------+---------------+--------+--------------+-------------------+\n",
      "|     Date|        Time (Local)|Eyeballs|Zeroes|Completed Trips|Requests|Unique Drivers|           datetime|\n",
      "+---------+--------------------+--------+------+---------------+--------+--------------+-------------------+\n",
      "|10-Sep-12| 10-Sep-2012 7:00:00|       5|     0|              2|       2|             9|2012-09-10 07:00:00|\n",
      "|10-Sep-12| 10-Sep-2012 8:00:00|       6|     0|              2|       2|            14|2012-09-10 08:00:00|\n",
      "|10-Sep-12| 10-Sep-2012 9:00:00|       8|     3|              0|       0|            14|2012-09-10 09:00:00|\n",
      "|10-Sep-12|10-Sep-2012 10:00:00|       9|     2|              0|       1|            14|2012-09-10 10:00:00|\n",
      "|10-Sep-12|10-Sep-2012 11:00:00|      11|     1|              4|       4|            11|2012-09-10 11:00:00|\n",
      "|10-Sep-12|10-Sep-2012 12:00:00|      12|     0|              2|       2|            11|2012-09-10 12:00:00|\n",
      "|10-Sep-12|10-Sep-2012 13:00:00|       9|     1|              0|       0|             9|2012-09-10 13:00:00|\n",
      "|10-Sep-12|10-Sep-2012 14:00:00|      12|     1|              0|       0|             9|2012-09-10 14:00:00|\n",
      "|10-Sep-12|10-Sep-2012 15:00:00|      11|     2|              1|       2|             7|2012-09-10 15:00:00|\n",
      "|10-Sep-12|10-Sep-2012 16:00:00|      11|     2|              3|       4|             6|2012-09-10 16:00:00|\n",
      "|10-Sep-12|10-Sep-2012 17:00:00|      12|     2|              3|       4|             4|2012-09-10 17:00:00|\n",
      "|10-Sep-12|10-Sep-2012 18:00:00|      11|     1|              3|       4|             7|2012-09-10 18:00:00|\n",
      "|10-Sep-12|10-Sep-2012 19:00:00|      13|     2|              2|       3|             7|2012-09-10 19:00:00|\n",
      "|10-Sep-12|10-Sep-2012 20:00:00|      11|     1|              0|       0|             5|2012-09-10 20:00:00|\n",
      "|10-Sep-12|10-Sep-2012 21:00:00|      11|     0|              1|       1|             3|2012-09-10 21:00:00|\n",
      "|10-Sep-12|10-Sep-2012 22:00:00|      16|     3|              0|       2|             4|2012-09-10 22:00:00|\n",
      "|10-Sep-12|10-Sep-2012 23:00:00|      21|     5|              3|       3|             4|2012-09-10 23:00:00|\n",
      "|11-Sep-12| 11-Sep-2012 0:00:00|       9|     3|              1|       1|             3|2012-09-11 00:00:00|\n",
      "|11-Sep-12| 11-Sep-2012 1:00:00|       3|     2|              0|       1|             3|2012-09-11 01:00:00|\n",
      "|11-Sep-12| 11-Sep-2012 2:00:00|       1|     1|              0|       0|             1|2012-09-11 02:00:00|\n",
      "+---------+--------------------+--------+------+---------------+--------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uber.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|              window|Total Completed Trips|\n",
      "+--------------------+---------------------+\n",
      "|{2012-09-21 19:00...|                  256|\n",
      "|{2012-09-22 19:00...|                  189|\n",
      "|{2012-09-14 19:00...|                  181|\n",
      "|{2012-09-15 19:00...|                  161|\n",
      "|{2012-09-20 19:00...|                  130|\n",
      "|{2012-09-11 19:00...|                   96|\n",
      "|{2012-09-13 19:00...|                   63|\n",
      "|{2012-09-16 19:00...|                   51|\n",
      "|{2012-09-17 19:00...|                   47|\n",
      "|{2012-09-19 19:00...|                   46|\n",
      "|{2012-09-18 19:00...|                   42|\n",
      "|{2012-09-12 19:00...|                   40|\n",
      "|{2012-09-10 19:00...|                   24|\n",
      "|{2012-09-09 19:00...|                   20|\n",
      "|{2012-09-23 19:00...|                   19|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, window\n",
    "\n",
    "\n",
    "# Group the data by 24-hour windows and sum the completed trips\n",
    "completed_trips_by_window = uber \\\n",
    "    .groupBy(window(\"datetime\", \"24 hours\")) \\\n",
    "    .agg(sum(\"Completed Trips\").alias(\"Total Completed Trips\")) \\\n",
    "    .orderBy(\"Total Completed Trips\", ascending=False)\n",
    "\n",
    "\n",
    "completed_trips_by_window.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the highest number of completed trips within a 24-hour period\n",
    "highest_completed_trips_in_24_hours = completed_trips_by_window \\\n",
    "    .select(\"Total Completed Trips\") \\\n",
    "    .first()[\"Total Completed Trips\"]\n",
    "\n",
    "print(highest_completed_trips_in_24_hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hour with the most requests is: 23\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, sum\n",
    "\n",
    "hourly_requests = uber.groupBy(hour(\"datetime\").alias(\"hour\")).agg(sum(\"Requests\").alias(\"total_requests\")).orderBy(\"total_requests\", ascending=False)\n",
    "\n",
    "most_requested_hour = hourly_requests.select(\"hour\").first()[0]\n",
    "print(\"The hour with the most requests is:\", most_requested_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
